{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"Home"},{"location":"AWS-interview-questions/","text":"AWS Interview Questions \u00b6 What is the difference between AMI and Instance? AMI is a template of an image (Windows, Ubuntu, etc.,) Many different tyes of instances can be launched from one AMI. An instance type determines the hardware of the host computer used for your instance Scalability vs Elasticity? Scalability - Increase of system resources Elasticity - Increase in machines Which AWS offerning enables customer to find, buy, and immediately start using software solutions in their AWS environment? a. AWS Config b. AWS OpsWorks c. AWS SDK d. AWS Marketplace (Correct ans) Resilient Architecture In what scenarios should you use Clasic load balancer and application load balancer? Why d you use Elasticcache? and in what cases? When we have increase the performance of the system when a similar data is cached very often How to track access of S3 bucket? Enable cloud trail to audit all amazon S3 bucket","title":"AWS Interview Questions"},{"location":"AWS-interview-questions/#aws-interview-questions","text":"What is the difference between AMI and Instance? AMI is a template of an image (Windows, Ubuntu, etc.,) Many different tyes of instances can be launched from one AMI. An instance type determines the hardware of the host computer used for your instance Scalability vs Elasticity? Scalability - Increase of system resources Elasticity - Increase in machines Which AWS offerning enables customer to find, buy, and immediately start using software solutions in their AWS environment? a. AWS Config b. AWS OpsWorks c. AWS SDK d. AWS Marketplace (Correct ans) Resilient Architecture In what scenarios should you use Clasic load balancer and application load balancer? Why d you use Elasticcache? and in what cases? When we have increase the performance of the system when a similar data is cached very often How to track access of S3 bucket? Enable cloud trail to audit all amazon S3 bucket","title":"AWS Interview Questions"},{"location":"Terraform-Interview-questions/","text":"Terraform Interview quetions \u00b6 Define Terraform. Answer: The first entry among Terraform interview questions always deals with the definition of Terraform. Terraform is a tool for creating, changing, and versioning infrastructure with higher safety and efficiency. Terraform is now popular all over the world as an important addition to the chain of important DevOps tools. Terraform provides essential functionalities of managing solutions for in-house issues. The facility of \u2018infrastructure as a code\u2019 model in Terraform is also one of the popular reasons for its adoption. What are the reasons to choose Terraform for DevOps? Answer: Candidates would definitely encounter this mention among top Terraform interview questions. The foremost reason to choose Terraform for DevOps is evident in the improvement of quality and efficiency in software delivery. Terraform supports automation and helps in running infrastructure as code. Another potential reason for choosing Terraform is the facility for implementing almost any type of coding principle. Also Read: Top 30 Kubernetes Interview Questions What are the notable features of Terraform? Answer: The features of Terraform would be one of the common topics of the latest Terraform interview questions. The key features of Terraform are as follows. In-built graphing features for visualization of infrastructure Friendly custom syntax helps in improving efficiency The ability for understanding resource relationships Contribution of updates and new features by the Open Source Project Capability for breaking down a configuration into smaller parts for ease of organization and maintenance 4. How does Terraform work? Answer: The working of Terraform is a formidable topic that churns out many relevant Terraform interview questions. The best answer to this question would be to point towards the plugin-based architecture of Terraform. The plugin-based architecture helps developers in extending functionalities of Terraform. Developers could write new plugins or compile the modified versions of current plugins. What are the notable applications of Terraform? Answer: The use cases of Terraform are also important aspects of the best Terraform interview questions. Generally, the applications of Terraform are very broad due to the facility for extending the abilities of Terraform for resource manipulation. Here are some of the notable applications of Terraform. Heroku App setup Self-service clusters Development of multi-tier applications Creation of disposable environments Multi-cloud deployment Resource schedulers Developing software demos Ansible and Terraform are the two names that are prominent in the DevOps landscape now. Let\u2019s understand the Ansible vs Terraform battle! What is the process for making the object of one module available for another module at a higher level? Answer: This entry can be one of the difficult Terraform interview questions. Here is the process to make an object of one module available for another module at a higher level. Define an output variable in a resource configuration. Declare the output variable of module_1 for use in another module\u2019s configuration Create a new key name and ensure that it is equivalent to the output variable of module_1 Now, create a file \u2018variable.tf\u2019 for module_2 Set up an input variable inside the \u2018variable.tf\u2019 the file that would enable dynamic configuration of the resource in a module In order to ensure the availability of the variable to another module, repeat the process again. The reason for this is the restricted scope of the particular variable to module_2. Do you know about the new factors in the latest v1.24.0 and v1.25.0 Terraform Azure Provider? Answer: Candidates should prepare for such Terraform technical interview questions. You can find multiple new resources in the latest versions of Terraform alongside new data resources. For example, Azurerm_batch_certificate. The new resource can support the management of certificates in the Azure batch. Furthermore, it also helps in the management of public IP and the prefix in networking. Another data resource in the latest versions is the Azurerm_firewall that helps in accessing data for a particular firewall existing already. In addition, the new versions also involve a lot of bug fixes. You can also find improvement in the azurerm_app_service resource in the latest versions. Does Terraform support themes? Answer: Candidates could land up with this entry among Terraform technical interview questions generally. The answer implies that the v0.3.1 of Terraform supports Gtk themes. We can use the command \u201ccp/usr/wherever/THEMENAME/gtk/gtkrc $HOME/.gtkrc\u201d for enabling gtk theme in a system. If the command shows error in opening the theme files or in the event of failure of the files to open, edits in the .gtkrc are mandatory. After editing, you have to attach the line \u201cpixmap_path/usr/wherever/THEMENAME/gtk\u201d at the start of the file name. Now, the theme could load at startup. What are the components of Terraform? Answer: The structure of Terraform is another notable point for the best Terraform interview questions. The logical division of Terraform into distinct structures refers to two distinct components. The two components are the Terraform Core and Terraform Plugins. The Terraform Core utilizes remote procedure calls (RPCs) for communicating with Terraform Plugins. In addition, Terraform Core also offers diverse ways of discovering and loading plugins according to requirements. The Terraform Plugins represent an implementation for a specific service such as bash or AWS or provisioner. What are the primary responsibilities of Terraform Core? Answer: This is one of the basic Terraform interview questions that you can face. The Terraform Core is a statically-compiled binary written by using the Go programming language. The compiled binary offers an entry-point for Terraform users. The primary responsibilities of the Terraform Core are as follows. Resource state management Execution of plans Communication with plugins through RPC Construction of Resource Graph Infrastructure as code functionalities for reading and interpolation of configuration files and modules Check: Most Common Jenkins Interview Questions & Answers What is the Terraform Plugins? Answer: Candidates should prepare for Terraform interview questions based on this topic. Terraform Plugins are executable binaries written in Go programming language. Plugins are basically the providers and provisioners in Terraform configurations. Terraform has various in-built provisioner plugins, and users have to discover provider plugins dynamically according to their requirements. The Terraform plugins help in domain-specific implementation of the service they represent. What are the primary responsibilities of the provider and provisioner plugins? Answer: Candidates could find this entry as a follow up to Terraform interview questions on architecture or the Terraform plugins. The primary responsibilities for provider plugins are as follows. Authentication with infrastructure provider Definition of resources that map to particular services Initialization of libraries used for making API calls The primary responsibility of provisioner plugins is the execution of commands or scripts on a specific resource after creation or upon its destruction. How does Terraform help in discovering plugins? Answer: This entry is one of the most popular Terraform interview questions. The command \u201cterraform init\u201d helps Terraform read configuration files in the working directory. Then, Terraform finds out the necessary plugins and searches for installed plugins in different locations. In addition, Terraform also downloads additional plugins at times. Then, it decides the plugin versions for using and writes a lock file for ensuring that Terraform will use the same plugin versions. What are the different behaviors of Terraform plugins during discovery? Answer: This question is one of the tricky Terraform interview questions that can confuse many expert candidates. The behavior of the plugins depends on their type. The three kinds of plugins are built-in provisioners, providers by HashiCorp, and third-party providers and provisioners. The in-built provisioner plugins are always available in the Terraform binary. The providers by HashiCorp download automatically if not installed already. Regarding the third-party providers and provisioners, you have to install them manually. What is the Terraform configuration for creating a single EC2 instance on AWS? Answer: Candidates could land up with this interesting entry among Terraform DevOps interview questions. The following Terraform configuration helps in creating a single EC2 instance on AWS. provider \"aws\" { region = \"ap-south-1\" } resource \"aws_instance\" \"example\" { ami = \"ami-4fc58420\" instance_type = \"t2.micro\" tags { Name = \"terraform-example\" } } It is required to be fully prepared before going for a DevOps interview. Prepare with these Top DevOps Interview Questions to ace the interview! Why does POVRay render fields and does not display sometimes? Answer: This is one of the critical Terraform DevOps interview questions. The primary reason for the failure of default export to POVRay could be the lower version of POVRay. Version 3.0 of POVRay does not support the display. The same reason could be evident in the case of failure in a display of POVRay without error reports. Terraform works effectively with the 3.1 version of POVRay. So, you should use the \u2013pov30 switch for informing Terraform about the issue. You can check the version of POVRay by typing \u201cPOVRay\u201d and observing the first line of the output. How can I check if the POVRay install is compatible with Terraform? Answer: Candidates could find tough Terraform interview questions like this one. You can try \u201cpovray+l tf_land.pov\u201d to check whether the POVRay installs on your system is ok with Terraform. You can find two outcomes \u2013 one good and one bad. The good message is an error \u201ctf_land.pov:26:error: Error opening TGA image\u201d. The \u201ctf_land.pov\u201d denotes the file distributed in the root directory of terraforming. It means that you can find the included file in the POV on your system. The second message is about \u2018colors.inc,\u2019 that indicates the absence of files in your POV. As a result, you can clearly find out whether the POVRay installs works with Terraform or not. What if I encounter a serious error and want to rollback? Answer: Candidates should find this entry among practical Terraform interview questions. The answer is recommitting the previous version of the code for making it the new and current version in a VCS (Version Control System). As a result, a terraform run triggers and runs the old code. It is essential to ensure that the old code contains all entities provisioned in the code for rollback. If the state file has been subject to corruption from a recent Terraform run, then you can opt for State Rollback Feature in Terraform Enterprise. It can help you to roll back to the previous latest state. The validation for this process is the versioning of every state change. Can I add policies to the open-source or Pro version of Terraform Enterprise? Answer: This is one of the most popular Terraform interview questions coming up in recent interviews. First of all, you cannot add policies to the open-source version of Terraform Enterprise. The same also goes for the Enterprise Pro version. The Premium version of Terraform Enterprise only could access the sentinel policies. What are the ways to lock Terraform module versions? Answer: Candidates could find such technical Terraform interview questions difficult. Preparing for such questions in advance gives candidates a definite advantage. The answer is that there is a proven way of locking Terraform module versions. Using the Terraform module registry as a source, you can use the \u2018version\u2019 attribute in the module in the Terraform configuration file. Using a GitHub repository as a source, you have to specify branch, versions, and query string with \u2018?ref\u2019. Are callbacks possible with Terraform on Azure? Answer: This is also one of the new Terraform interview questions for aspiring candidates. Callbacks are possible on Azure with Terraform by using the Azure Event Hubs. AzureRM provider of Terraform provides this functionality easily to users. Most important of all, Azure Cloud Shell of Microsoft provides an already installed instance of Terraform. If you\u2019re a cloud professional preparing for an Azure interview, we recommend you to prepare with the top Azure interview questions. Can I use Terraform for on-premises infrastructure? Answer: Candidates could find this question as one of the tough ones based on real experience. The answer to this question clearly implies the feasibility of using Terraform with on-premises infrastructure. Many providers offer this functionality, and you can choose one according to your requirements. Certain providers also have APIs for accessing Terraform in on-premises infrastructure. What are the version controls supported on Terraform? Answer: The candidate could find this entry as one of the important Terraform interview questions. Although this question may seem to be the simplest of the lot, it is essential to point out the precise responses. First of all, GitHub is the basic version control supported on Terraform. In addition, you can also find the support of GitLab CE and GitLab EE on Terraform. Furthermore, Terraform also supports the Bucket Cloud. Is there any similarity between the management of Azure Availability Zones and management by other available cloud providers? Answer: The response to this question would directly refer to the fact that availability zones have different areas and zones. Every availability zone has a specific power source and network. Any region with an enabled availability zone would have three different availability zones. You need to note that AzureRM Terraform provider does not have any accessible resources for the management of Azure Availability Zones. However, this is the present scenario, and AzureRM Terraform provider could include some improvements in the future for this issue. How can I upgrade plugins on Terraform? Answer: The answer would start off with running \u2018terraform init\u2019 with the \u2018-upgrade\u2019 option. The command helps in rechecking the releases.hashicorp.com to find out new acceptable provider versions. The command also downloads the available provider versions. Such types of actions are evident in the case of providers which have their acceptable versions in the automatic downloads directory. The automatic downloads directory is \u201c.terraform/plugins/ _ .\u201d In case of installation of any acceptable version of a specific provider in another location, the \u2018terraform init -upgrade\u2019 command will not download a new version. Tip Chef is one of the top DevOps tools. If you are going for a DevOps interview, don\u2019t forget to check out these top Chef interview questions and answers!","title":"Terraform Interview quetions"},{"location":"Terraform-Interview-questions/#terraform-interview-quetions","text":"Define Terraform. Answer: The first entry among Terraform interview questions always deals with the definition of Terraform. Terraform is a tool for creating, changing, and versioning infrastructure with higher safety and efficiency. Terraform is now popular all over the world as an important addition to the chain of important DevOps tools. Terraform provides essential functionalities of managing solutions for in-house issues. The facility of \u2018infrastructure as a code\u2019 model in Terraform is also one of the popular reasons for its adoption. What are the reasons to choose Terraform for DevOps? Answer: Candidates would definitely encounter this mention among top Terraform interview questions. The foremost reason to choose Terraform for DevOps is evident in the improvement of quality and efficiency in software delivery. Terraform supports automation and helps in running infrastructure as code. Another potential reason for choosing Terraform is the facility for implementing almost any type of coding principle. Also Read: Top 30 Kubernetes Interview Questions What are the notable features of Terraform? Answer: The features of Terraform would be one of the common topics of the latest Terraform interview questions. The key features of Terraform are as follows. In-built graphing features for visualization of infrastructure Friendly custom syntax helps in improving efficiency The ability for understanding resource relationships Contribution of updates and new features by the Open Source Project Capability for breaking down a configuration into smaller parts for ease of organization and maintenance 4. How does Terraform work? Answer: The working of Terraform is a formidable topic that churns out many relevant Terraform interview questions. The best answer to this question would be to point towards the plugin-based architecture of Terraform. The plugin-based architecture helps developers in extending functionalities of Terraform. Developers could write new plugins or compile the modified versions of current plugins. What are the notable applications of Terraform? Answer: The use cases of Terraform are also important aspects of the best Terraform interview questions. Generally, the applications of Terraform are very broad due to the facility for extending the abilities of Terraform for resource manipulation. Here are some of the notable applications of Terraform. Heroku App setup Self-service clusters Development of multi-tier applications Creation of disposable environments Multi-cloud deployment Resource schedulers Developing software demos Ansible and Terraform are the two names that are prominent in the DevOps landscape now. Let\u2019s understand the Ansible vs Terraform battle! What is the process for making the object of one module available for another module at a higher level? Answer: This entry can be one of the difficult Terraform interview questions. Here is the process to make an object of one module available for another module at a higher level. Define an output variable in a resource configuration. Declare the output variable of module_1 for use in another module\u2019s configuration Create a new key name and ensure that it is equivalent to the output variable of module_1 Now, create a file \u2018variable.tf\u2019 for module_2 Set up an input variable inside the \u2018variable.tf\u2019 the file that would enable dynamic configuration of the resource in a module In order to ensure the availability of the variable to another module, repeat the process again. The reason for this is the restricted scope of the particular variable to module_2. Do you know about the new factors in the latest v1.24.0 and v1.25.0 Terraform Azure Provider? Answer: Candidates should prepare for such Terraform technical interview questions. You can find multiple new resources in the latest versions of Terraform alongside new data resources. For example, Azurerm_batch_certificate. The new resource can support the management of certificates in the Azure batch. Furthermore, it also helps in the management of public IP and the prefix in networking. Another data resource in the latest versions is the Azurerm_firewall that helps in accessing data for a particular firewall existing already. In addition, the new versions also involve a lot of bug fixes. You can also find improvement in the azurerm_app_service resource in the latest versions. Does Terraform support themes? Answer: Candidates could land up with this entry among Terraform technical interview questions generally. The answer implies that the v0.3.1 of Terraform supports Gtk themes. We can use the command \u201ccp/usr/wherever/THEMENAME/gtk/gtkrc $HOME/.gtkrc\u201d for enabling gtk theme in a system. If the command shows error in opening the theme files or in the event of failure of the files to open, edits in the .gtkrc are mandatory. After editing, you have to attach the line \u201cpixmap_path/usr/wherever/THEMENAME/gtk\u201d at the start of the file name. Now, the theme could load at startup. What are the components of Terraform? Answer: The structure of Terraform is another notable point for the best Terraform interview questions. The logical division of Terraform into distinct structures refers to two distinct components. The two components are the Terraform Core and Terraform Plugins. The Terraform Core utilizes remote procedure calls (RPCs) for communicating with Terraform Plugins. In addition, Terraform Core also offers diverse ways of discovering and loading plugins according to requirements. The Terraform Plugins represent an implementation for a specific service such as bash or AWS or provisioner. What are the primary responsibilities of Terraform Core? Answer: This is one of the basic Terraform interview questions that you can face. The Terraform Core is a statically-compiled binary written by using the Go programming language. The compiled binary offers an entry-point for Terraform users. The primary responsibilities of the Terraform Core are as follows. Resource state management Execution of plans Communication with plugins through RPC Construction of Resource Graph Infrastructure as code functionalities for reading and interpolation of configuration files and modules Check: Most Common Jenkins Interview Questions & Answers What is the Terraform Plugins? Answer: Candidates should prepare for Terraform interview questions based on this topic. Terraform Plugins are executable binaries written in Go programming language. Plugins are basically the providers and provisioners in Terraform configurations. Terraform has various in-built provisioner plugins, and users have to discover provider plugins dynamically according to their requirements. The Terraform plugins help in domain-specific implementation of the service they represent. What are the primary responsibilities of the provider and provisioner plugins? Answer: Candidates could find this entry as a follow up to Terraform interview questions on architecture or the Terraform plugins. The primary responsibilities for provider plugins are as follows. Authentication with infrastructure provider Definition of resources that map to particular services Initialization of libraries used for making API calls The primary responsibility of provisioner plugins is the execution of commands or scripts on a specific resource after creation or upon its destruction. How does Terraform help in discovering plugins? Answer: This entry is one of the most popular Terraform interview questions. The command \u201cterraform init\u201d helps Terraform read configuration files in the working directory. Then, Terraform finds out the necessary plugins and searches for installed plugins in different locations. In addition, Terraform also downloads additional plugins at times. Then, it decides the plugin versions for using and writes a lock file for ensuring that Terraform will use the same plugin versions. What are the different behaviors of Terraform plugins during discovery? Answer: This question is one of the tricky Terraform interview questions that can confuse many expert candidates. The behavior of the plugins depends on their type. The three kinds of plugins are built-in provisioners, providers by HashiCorp, and third-party providers and provisioners. The in-built provisioner plugins are always available in the Terraform binary. The providers by HashiCorp download automatically if not installed already. Regarding the third-party providers and provisioners, you have to install them manually. What is the Terraform configuration for creating a single EC2 instance on AWS? Answer: Candidates could land up with this interesting entry among Terraform DevOps interview questions. The following Terraform configuration helps in creating a single EC2 instance on AWS. provider \"aws\" { region = \"ap-south-1\" } resource \"aws_instance\" \"example\" { ami = \"ami-4fc58420\" instance_type = \"t2.micro\" tags { Name = \"terraform-example\" } } It is required to be fully prepared before going for a DevOps interview. Prepare with these Top DevOps Interview Questions to ace the interview! Why does POVRay render fields and does not display sometimes? Answer: This is one of the critical Terraform DevOps interview questions. The primary reason for the failure of default export to POVRay could be the lower version of POVRay. Version 3.0 of POVRay does not support the display. The same reason could be evident in the case of failure in a display of POVRay without error reports. Terraform works effectively with the 3.1 version of POVRay. So, you should use the \u2013pov30 switch for informing Terraform about the issue. You can check the version of POVRay by typing \u201cPOVRay\u201d and observing the first line of the output. How can I check if the POVRay install is compatible with Terraform? Answer: Candidates could find tough Terraform interview questions like this one. You can try \u201cpovray+l tf_land.pov\u201d to check whether the POVRay installs on your system is ok with Terraform. You can find two outcomes \u2013 one good and one bad. The good message is an error \u201ctf_land.pov:26:error: Error opening TGA image\u201d. The \u201ctf_land.pov\u201d denotes the file distributed in the root directory of terraforming. It means that you can find the included file in the POV on your system. The second message is about \u2018colors.inc,\u2019 that indicates the absence of files in your POV. As a result, you can clearly find out whether the POVRay installs works with Terraform or not. What if I encounter a serious error and want to rollback? Answer: Candidates should find this entry among practical Terraform interview questions. The answer is recommitting the previous version of the code for making it the new and current version in a VCS (Version Control System). As a result, a terraform run triggers and runs the old code. It is essential to ensure that the old code contains all entities provisioned in the code for rollback. If the state file has been subject to corruption from a recent Terraform run, then you can opt for State Rollback Feature in Terraform Enterprise. It can help you to roll back to the previous latest state. The validation for this process is the versioning of every state change. Can I add policies to the open-source or Pro version of Terraform Enterprise? Answer: This is one of the most popular Terraform interview questions coming up in recent interviews. First of all, you cannot add policies to the open-source version of Terraform Enterprise. The same also goes for the Enterprise Pro version. The Premium version of Terraform Enterprise only could access the sentinel policies. What are the ways to lock Terraform module versions? Answer: Candidates could find such technical Terraform interview questions difficult. Preparing for such questions in advance gives candidates a definite advantage. The answer is that there is a proven way of locking Terraform module versions. Using the Terraform module registry as a source, you can use the \u2018version\u2019 attribute in the module in the Terraform configuration file. Using a GitHub repository as a source, you have to specify branch, versions, and query string with \u2018?ref\u2019. Are callbacks possible with Terraform on Azure? Answer: This is also one of the new Terraform interview questions for aspiring candidates. Callbacks are possible on Azure with Terraform by using the Azure Event Hubs. AzureRM provider of Terraform provides this functionality easily to users. Most important of all, Azure Cloud Shell of Microsoft provides an already installed instance of Terraform. If you\u2019re a cloud professional preparing for an Azure interview, we recommend you to prepare with the top Azure interview questions. Can I use Terraform for on-premises infrastructure? Answer: Candidates could find this question as one of the tough ones based on real experience. The answer to this question clearly implies the feasibility of using Terraform with on-premises infrastructure. Many providers offer this functionality, and you can choose one according to your requirements. Certain providers also have APIs for accessing Terraform in on-premises infrastructure. What are the version controls supported on Terraform? Answer: The candidate could find this entry as one of the important Terraform interview questions. Although this question may seem to be the simplest of the lot, it is essential to point out the precise responses. First of all, GitHub is the basic version control supported on Terraform. In addition, you can also find the support of GitLab CE and GitLab EE on Terraform. Furthermore, Terraform also supports the Bucket Cloud. Is there any similarity between the management of Azure Availability Zones and management by other available cloud providers? Answer: The response to this question would directly refer to the fact that availability zones have different areas and zones. Every availability zone has a specific power source and network. Any region with an enabled availability zone would have three different availability zones. You need to note that AzureRM Terraform provider does not have any accessible resources for the management of Azure Availability Zones. However, this is the present scenario, and AzureRM Terraform provider could include some improvements in the future for this issue. How can I upgrade plugins on Terraform? Answer: The answer would start off with running \u2018terraform init\u2019 with the \u2018-upgrade\u2019 option. The command helps in rechecking the releases.hashicorp.com to find out new acceptable provider versions. The command also downloads the available provider versions. Such types of actions are evident in the case of providers which have their acceptable versions in the automatic downloads directory. The automatic downloads directory is \u201c.terraform/plugins/ _ .\u201d In case of installation of any acceptable version of a specific provider in another location, the \u2018terraform init -upgrade\u2019 command will not download a new version. Tip Chef is one of the top DevOps tools. If you are going for a DevOps interview, don\u2019t forget to check out these top Chef interview questions and answers!","title":"Terraform Interview quetions"},{"location":"interviewQuestions/","text":"CICD Interview Questions \u00b6 We believe that these questions/exercise will help the candidates to get the right expectations about the type of work that (s)he is goin to get. This will also help us to fitler candidates who are not suitable for the position. **Try to answer maximum questions. Feel free to use Google. Please upload the answers to a github repository and share us the URL. Configure Jenkins job to trigger SonarQube analysis on Pull Requests/Code Reviews? Write a Jenkins pipelin to fetch source from GitHub and publish the (Java/NodeJS) artifacts to Artifactory/nexus repository. Create a container(Docker) image that contains an application server with above build artifacts. Define Deployment, Service yaml/json files to deploy above container image on to Kubernetes/OpenShift platforms. Write a Cookbook/Playbook to host a simple application server with above build artifacts (Question 2). Data Flow? Branching Statergy \u00b6 common libraries in Jenkins Multi branch pipeline vs Org scan job HOW TO DO SECURITY? LEAST PRIVILEGE Handling \"Least privileges\" concepts helps you manage the AAA concepts: Authentication\u2009\u2014\u2009Control who can access the system Authorization\u2009\u2014\u2009Control what each user can do on the system Accounting\u2009\u2014\u2009Monitor your system to ensure that only valid processes are executing HOW JENKINS EXECUTES A PIPELINE \u00b6 A simple overview of how Jenkins executes a Pipeline helps us understand the security considerations: By default, the Pipeline executes with the full privileges of the Jenkins administrator You can configure Jenkins to execute Pipelines with fewer privileges All of the Pipeline logic, the Groovy conditionals, loops, etc execute on the master Creates a workspace for each build that runs Stores files created for that build The Pipeline calls steps that run on agents WHAT AGENTS DO \u00b6 The Pipeline calls a series of steps, each of which is a script or command that does the real work and mostly executes using an executor on an agent Agent writes some files to the local node Agent sends data back to the master Agent can also request information from the master What is Declarity pipeline vs scripted pipeline Scripted: sequential execution, using Groovy expressions for flow control Declarative: uses a framework to control execution Explain Jenkins Pipeline JENKINS PIPELINE Jenkins Pipeline is a tool for defining your Continuous Delivery/Deployment flow as code New fundamental \"type\" in Jenkins that conceptualizes and models a Continuous Delivery process Two syntaxes: Scripted: sequential execution, using Groovy expressions for flow control Declarative: uses a framework to control execution Uses the Pipeline DSL (Domain-specific Language) programmatically manipulate Jenkins Objects Captures the entire continuous delivery process as code Pipeline is not a job creation tool like the Job DSL plugin or Jenkins Job Builder What are pipeline benefits PIPELINE BENEFITS (1/2) The pipeline functionality is: Durable: The Jenkins master can restart and the Pipeline continues to run Pausable: can stop and wait for human input or approval Versatile: supports complex real-world CD requirements (fork, join, loop, parallelize) Extensible: supports custom extensions to its \"DSL\" (Domain-specific Language) PIPELINE BENEFITS (2/2) Reduces number of jobs Easier maintenance Decentralization of job configuration Easier specification through code JENKINS VOCABULARY Master: Computer, VM or container where Jenkins is installed and run Serves requests and handles build tasks Agent: (formerly \"slave\") Computer, VM or container that connects to a Jenkins Master Executes tasks when directed by the Master Has a number and scope of operations to perform. Node is sometimes used to refer to the computer, VM or container used for the Master or Agent; be careful because \"Node\" has another meaning for Pipeline Executor: Computational resource for running builds Performs Operations Can run on any Master or Agent, although running builds on masters can degrade performance and opens up serious security vulnerabilities Can be parallelized on a specific Master or Agent JENKINS PIPELINE SECTIONS The Jenkinsfile that defines a Pipeline uses a DSL based on Apache Groovy syntax Is structured in sections, called stages Each stage includes steps steps include the actual tests/commands to run An agent defines where the programs and scripts execute This is illustrated in the following simple Jenkinsfile: pipeline { agent { label 'linux' } stages { stage ( 'MyBuild' ) { steps { sh './jenkins/build.sh' } } stage ( 'MySmalltest' ) { steps { sh './jenkins/smalltest.sh' } } } } DECLARATIVE VERSUS SCRIPTED PIPELINE Two different syntaxes for Pipeline Both are built on same Pipeline subsystem Both definitions are stored in a Jenkinsfile under SCM (\"Pipeline-as-code\") Both rely on the Pipeline DSL, which is based on Apache Groovy syntax Both can use steps built into Pipeline or provided in plugins Both support shared libraries SCRIPTED PIPELINE Executed serially, from top down Relies on Groovy expressions for flow control Requires extensive knowledge of Groovy syntax Very flexible and extensible Limitations are mostly Groovy limitations Power users with complex requirements may need it Novice users can easily mess up the syntax DECLARATIVE PIPELINE Stricter, pre-defined structure Execution can always be resumed after an interruption, no matter what caused the interruption Requires only limited knowledge of Groovy syntax Using Blue Ocean simplifies the Pipeline creation process even more Encourages a declarative programming model Reduces the risk of syntax errors Use the script step to include bits of scripted code within a Declarative Pipeline only when you have needs that are beyond the capabilities of Declarative syntax PIPELINE CONCEPTS Pipeline is \"glue\" code that binds tasks together into a CD flow. Use the Pipeline DSL for CI/CD scripting only! Do not code arbitrary networking and computational tasks into the Pipeline itself Use tools like Maven, Gradle, NPM, Ant and Make to do the majority of the build \u201cwork\u201d Use scripts written in Shell, Batch, or Python for other related tasks Then call these executables as steps in your Pipeline Declarative Pipeline keeps the complex logic of individual steps separate from the Pipeline itself, making the Pipeline code clean to read What is Archive Artifacts post stage in Jenkins Pipeline Use the pipeline step archiveArtifacts Requires a pattern to know which artifacts to archive my-app.zip: The file my-app.zip, at the workspace\u2019s root images/*.png: All files with .png extension in the images folder at the workspace\u2019s root target/**/*.jar: All files with .jar extension, recursively under the target folder, at the workspace\u2019s root Archiving keeps those files in ${JENKINS_HOME} forever unless you delete them FINGERPRINTS A fingerprint is the MD5 checksum of an artifact Each archived artifact can be fingerprinted; merely check the \"Fingerprint\" box when you create the archiving step Jenkins uses Fingerprints to keep track of artifacts without any ambiguity A database of all fingerprints is managed on the Master Located in the ${JENKINS_HOME}/fingerprints directory USING ARCHIVED ARTIFACTS In a production environment, your build chain system (Maven, Gradle, Make, etc.) publishes artifacts to an artifact repository such as Artifactory, Nexus, etc. Teams can also deploy artifacts from Jenkins to test environments Use the Copy Artifact Plugin to take artifacts from one project (Pipeline run) and copy them to another Project ARTIFACT RETENTION POLICY Coupled to build retention policy Deleting a build deletes attached artifacts Good practice: discard build and clean it Driven by age: # days to keep a build Driven by number: Max # of builds to keep Important builds can individually be Kept Forever WHAT IS JUNIT? As an external application, JUnit is a common testing framework for Java programs In the Jenkins context, JUnit is a publisher that consumes XML test reports Generates some graphical visualization of the historical test results Provides a web UI for viewing test reports, tracking failures and so forth Useful for tracking test result trends Works with all supported build tools You must specify the appropriate path name for the XML test reports generated by the build tool you are using JUnit in this course means the Jenkins publisher PARALLEL STAGES Stages can be run in parallel: Long running stages Builds for different target architectures or OSes (Debian/Fedora, Android/iOS) or different software versions (JVM8/11), et cetera Builds of independent modules of your code Unit and integration tests are tied to the code; run them in parallel DECLARATIVE PARALLEL SYNTAX Each \"parallelized branch\" is a stage A stage can use either steps or parallel at the top level; it cannot use both A stage within a parallel stage can contain agent and steps sections A parallel stage cannot contain agent or tools Add failfast true to force all parallel processes to abort if one fails HOW PARALLEL STAGES ARE SCHEDULED By default, Jenkins tries to allocate a stage to the last node on which it (or a similar stage) executed. This may mean that multiple parallel steps execute on the same node while other nodes in the cluster are idle Use the Least Load Plugin to replace the default load balancer with one that schedules a new stage to nodes with the least load SCRIPTED PIPELINES IN PRODUCTION This exercise is a short introduction to Scripted Pipelines Scripted Pipelines used in production have the following characteristics: Jenkinsfile must be created and modified in a text editor Should be Multibranch Pipeline Requires Git, Bitbucket or SVN Must be configured to the SCM repository, with credentials, hooks, etc Must be committed to the SCM like any other code WHAT IS MULTIBRANCH? Recommended for all new Pipelines All Pipelines created with Blue Ocean are multibranch Requires Git, Bitbucket or SVN Configures a set of branches Jenkins creates a subproject for each branch in the SCM repository WHY MULTIBRANCH Automatic Workflow creation for each new branch in the repository Assumes that webhooks are registered from the SCM to Jenkins Builds are specific to that child/branch and its unique SCM change and build history Automatic job pruning/deletion for branches deleted from the repository, according to the settings Ability to override the parent properties for a specific branch, if necessary SPECIFY DOCKER AGENT IN PIPELINE agent { docker } Execute the Pipeline or stage with the specified container The container will be dynamically provisioned on the node agent { dockerfile } Execute the Pipeline or stage with a container built from the specified Dockerfile DOCKER CONTAINER DURABILITY When you specify a docker container for an agent, Jenkins calls APIs directly These commands are serialized so they can resume after a master restart When you use a shell step to run a Docker command directly, the step is bound to the durable task of the shell The Docker container and any tasks running in it are terminated when the shell terminates SIMPLE SYNTAX Run all Pipeline stages on any node machine: ``` groovy pipeline { agent any .... } Run all Pipeline stages on a node machine tagged as bzzzmaven: pipeline { agent { label 'bzzzmaven' } .... } ``` Run all Pipeline stages on a container based on the image bzzzcentos:7: ``` groovy pipeline { agent { docker 'bzzzcentos:7' } .... } ``` PER-STAGE AGENT SYNTAX Do not run stages on an agent by default Run the stage Build on a node machine tagged as bzzzmaven Run the stage Deploy on a node machine tagged as bzzzproduction: pipeline { agent none stages { stage ( 'Build' ) { agent { label 'bzzzmaven' } steps { ... } } stage ( 'Deploy' ) { agent { label 'bzzzproduction' } steps { ... } } } } SPECIFY AGENTS FOR OUR PIPELINE We are building our application to work with both JDK7 and JDK8. This means that we need a JDK7 environment and a JDK8 environment. To implement this: * Change the global agent to be agent none; in other words, there is no global agent * Specify the appropriate agent (labeled either jdk7 or jdk8) for the build and test steps that need the specific JDK environment. * Specify agent any for stages that do not require a specific JDK version. STASH/UNSTASH Use stash to save a set of files for use later in the same build, but in another stage that executes on another node/workspace Use unstash to restore the stashed files into the current workspace of the other stage Stashed files are discarded at the end of the build Stashed files are archived in a compressed tar file, which works well for small (<5 Mb) files Stashing large files consumes significant resources (especially CPU) on both the Master and agent nodes For large files, consider using: the External Workspace manager plugin an external repository manager such as Nexus or Artifactory the Artifact Manager on S3 plugin (README) CALLING SYNTAX * stash and unstash are implemented as steps within a stage * stash requires one parameter, name, which is a simple identifier for the set of files being stashed * By default, stash places all workspace files into the named stash * To store files from a different directory (or a subset of the workspace): - Use the optional includes parameter to give the name of the files or directories to store. This accepts a set of Ant-style include patterns. - Other optional parameters are documented on the Pipeline: Basic Steps reference page * To unstash files: - Optionally, use the `dir` step to create a directory where the files will be written. - Call `unstash`, using the name you assigned with the stash step above. IMPLEMENT STASH/UNSTASH (\u00bd) * We are going to make our Pipeline build and test our software for both JDK 7 and JDK 8. - We will use stash/unstash to ensure that the JDK 7 tests are run against the JDK 7 build and that the JDK 8 tests are run against the JDK 8 build: * Add a step to the \"Build Java 7\" stage: \"Stash some files to be used later in the build\" - Name the stash \"Buzz Java 7\"; enter target/** in the \"Includes\" box to save everything under the target directory * Follow the same procedure to stash \"Buzz Java 8\" in the \"Build Java 8\" stage CODE: WAIT FOR INPUT The code in the Jenkinsfile looks like: stage ( 'Confirm Deploy to Staging' ) { agent none steps { input ( message: 'Deploy to Stage' , ok: \"Yes, let's do it!\" ) } } Note that Blue Ocean uses single quotes to enclose the text in the \"Message\" and \"OK\" boxes and automatically escapes apostrophes in the text To make the code cleaner, you can use the code editor to use double quotes to enclose the text Sample pipeline SAMPLE FINISHING PIPEINE pipeline { agent none stages { stage ( 'Fluffy Build' ) { parallel { stage ( 'Build Java 8' ) { agent { node { label 'java8' } } steps { sh './jenkins/build.sh' } post { success { stash ( name: 'Java 8' , includes: 'target/**' ) } } } stage ( 'Build Java 7' ) { agent { node { label 'java7' } } steps { sh './jenkins/build.sh' } post { success { archiveArtifacts 'target/*.jar' stash ( name: 'Java 7' , includes: 'target/**' ) } } } } } stage ( 'Fluffy Test' ) { parallel { stage ( 'Backend Java 8' ) { agent { node { label 'java8' } } steps { unstash 'Java 8' sh './jenkins/test-backend.sh' } post { always { junit 'target/surefire-reports/**/TEST*.xml' } } } stage ( 'Frontend' ) { agent { node { label 'java8' } } steps { unstash 'Java 8' sh './jenkins/test-frontend.sh' } post { always { junit 'target/test-results/**/TEST*.xml' } } } stage ( 'Performance Java 8' ) { agent { node { label 'java8' } } steps { unstash 'Java 8' sh './jenkins/test-performance.sh' } } stage ( 'Static Java 8' ) { agent { node { label 'java8' } } steps { unstash 'Java 8' sh './jenkins/test-static.sh' } } stage ( 'Backend Java 7' ) { agent { node { label 'java7' } } steps { unstash 'Java 7' sh './jenkins/test-backend.sh' } post { always { junit 'target/surefire-reports/**/TEST*.xml' } } } stage ( 'Frontend Java 7' ) { agent { node { label 'java7' } } steps { unstash 'Java 7' sh './jenkins/test-frontend.sh' } post { always { junit 'target/test-results/**/TEST*.xml' } } } stage ( 'Performance Java 7' ) { agent { node { label 'java7' } } steps { unstash 'Java 7' sh './jenkins/test-performance.sh' } } stage ( 'Static Java 7' ) { agent { node { label 'java7' } } steps { unstash 'Java 7' sh './jenkins/test-static.sh' } } } } stage ( 'Confirm Deploy' ) { when { branch 'master' } steps { input ( message: 'Okay to Deploy to Staging?' , ok: 'Let\\'s Do it!' ) } } stage ( 'Fluffy Deploy' ) { when { branch 'master' } agent { node { label 'java7' } } steps { unstash 'Java 7' sh \"./jenkins/deploy.sh ${params.DEPLOY_TO}\" } } } parameters { string ( name: 'DEPLOY_TO' , defaultValue: 'dev' , description: '' ) } } SAMPLE POST CODE pipeline { stages { stage ( 'Buzz Build' ) { parallel { stage ( 'Build Java 7' ) { steps { sh \"\"\" echo I am a $BUZZ_NAME! ./jenkins/build.sh \"\"\" } post { always { archiveArtifacts ( artifacts: 'target/*.jar' , fingerprint: true ) } success { stash ( name: 'Buzz Java 7' , includes: 'target/**' ) } } } ... } WHAT IS AN ENVIRONMENT DIRECTIVE? Specifies a sequence of key-value pairs that are defined as environment variables Can be specified globally for the Pipeline, to apply to all steps Can be specified for an individual stage to apply only to that stage By manually coding withEnv inside a steps block, a specific environment variable can be specified for one or more (but not all) steps within a stage SINGLE AND DOUBLE QUOTES The Pipeline DSL uses Apache Groovy syntax. Variables are dereferenced according to whether they are enclosed in single quotes or double quotes: Single quotes: The dereferencing syntax (${VARIABLE_NAME}) is passed literally to the Pipeline sh step; the shell interpreter on the agent dereferences the variable. Double quotes: The Pipeline code dereferences the variable on the master\u2019s JVM thread then passes the calculated string to the sh step. ENVIRONMENT EXAMPLE pipeline { agent any environment { CC = 'clang' } stages { stage('Example') { environment { AN_ACCESS_KEY = credentials('my-predefined-secret-text') } steps { sh 'printenv' The first use of environment applies to the entire Pipeline The second use of environment applies only to the Example stage NOTIFICATIONS EXAMPLE NOTIFICATIONS WHEN BUILD STARTS (\u00bd) stages { stage ( 'Start' ) { steps { // send build started notifications slackSend ( color: '#FFFF00' , message: \"STARTED: Job '${env.JOB_NAME} [${env.BUILD_NUMBER}]' (${env.BUILD_URL})\" ) } } } NOTIFICATIONS WHEN BUILD STARTS (2/2) /* ... unchanged ... */ // send to email emailext ( subject: \"STARTED: Job '${env.JOB_NAME} [${env.BUILD_NUMBER}]'\" , body: \"\"\"<p>STARTED: Job '${env.JOB_NAME} [${env.BUILD_NUMBER}]':</p> <p>Check console output at &QUOT;<a href='${env.BUILD_URL}'>${env.JOB_NAME} [${env.BUILD_NUMBER}]</a>&QUOT;</p>\"\"\" , recipientProviders: [[ $class : 'DevelopersRecipientProvider' ]] ) /* ... unchanged ... */ NOTIFICATIONS WHEN BUILD SUCCEEDS post { success { slackSend ( color: '#00FF00' , message: \"SUCCESSFUL: Job '${env.JOB_NAME} [${env.BUILD_NUMBER}]' (${env.BUILD_URL})\" ) emailext ( subject: \"SUCCESSFUL: Job '${env.JOB_NAME} [${env.BUILD_NUMBER}]'\" , body: \"\"\"<p>SUCCESSFUL: Job '${env.JOB_NAME} [${env.BUILD_NUMBER}]':</p> <p>Check console output at &QUOT;<a href='${env.BUILD_URL}'>${env.JOB_NAME} [${env.BUILD_NUMBER}]</a>&QUOT;</p>\"\"\" , recipientProviders: [[ $class : 'DevelopersRecipientProvider' ]] ) } } NOTIFICATIONS WHEN BUILD FAILS failure { slackSend ( color: '#FF0000' , message: \"FAILED: Job '${env.JOB_NAME} [${env.BUILD_NUMBER}]' (${env.BUILD_URL})\" ) emailext ( subject: \"FAILED: Job '${env.JOB_NAME} [${env.BUILD_NUMBER}]'\" , body: \"\"\"<p>FAILED: Job '${env.JOB_NAME} [${env.BUILD_NUMBER}]':</p> <p>Check console output at &QUOT;<a href='${env.BUILD_URL}'>${env.JOB_NAME} [${env.BUILD_NUMBER}]</a>&QUOT;</p>\"\"\" , recipientProviders: [[ $class : 'DevelopersRecipientProvider' ]] ) } CONFIGURE NOTIFICATIONS * Your Jenkins administrator must configure notifications for Jenkins in order for the notifications to be delivered. * Slack uses an API token - Integration points for each generate the token - That generated token must be copied into the Jenkins configuration * The Slack configuration provides a button that can be used to test the configured connection WHEN DIRECTIVE \u00b6 WHAT IS THE \"WHEN\" DIRECTIVE? \u00b6 Specifies conditions that must be met for Pipeline to execute the stage Must contain at least one condition Supports nested conditions BUILT-IN CONDITIONS \u00b6 branch \u2009\u2014\u2009Execute the stage when the branch being built matches the branch pattern given: when { branch 'master' } environment \u2009\u2014\u2009Execute the stage when the specified environment variable is set to the given value: when { environment name: 'DEPLOY_TO' , value: 'production' } expression \u2009\u2014\u2009Execute the stage when the specified expression evaluates to true: when { expression { return params . DEBUG_BUILD } } BUILT-IN NESTED CONDITIONS \u00b6 allOf\u2009\u2014\u2009Execute the stage when all nested conditions are true: when { allOf { branch 'master' environment name: 'DEPLOY_TO', value: 'production' // AND } } anyOf\u2009\u2014\u2009Execute the stage when at least one of the nested conditions is true when { anyOf { branch 'master' branch 'staging' // OR } } not\u2009\u2014\u2009Execute the stage when the nested condition is false. `when { not { branch 'master' } }` MULTIPLE CONDITIONS \u00b6 If the when directive contains more than one condition, all specified conditions must return true for the stage to execute. This is identical to using the allOf condition IMPLEMENTING \"WHEN\" DIRECTIVE \u00b6 The \"Confirm Deploy to Staging\" and \"Deploy to Staging\" steps are only appropriate when building from the master branch. Use Ctrl+S (Cmd+S on macOS) to open the Blue Ocean code editor and add when directives that specify the branch condition to these two stages. The Blue Ocean Editor does not show the when directive but it is in the Jenkinsfile. When we save and run the Pipeline, these steps are skipped because we are not working on the master branch. \"WHEN\" DIRECTIVE EXAMPLE \u00b6 ... stage ( 'Confirm Deploy to Staging' ) { when { branch 'master' } steps { input ( message: 'Deploy to Stage' , ok: 'Yes, let\\'s do it!' ) } } stage ( 'Deploy to Staging' ) { agent { node { label 'java8' } } when { branch 'master' } steps { unstash 'Buzz Java 8' sh './jenkins/deploy.sh staging' } } ... CREDENTIALS EXAMPLE CODE \u00b6 ... stage ( 'Deploy Reports' ) steps { ... withCredentials ( bindings: [ string ( credentialsId: 'my-elastic-key' , variable: 'ELASTIC_ACCESS_KEY' )]) { // Environment Variable available in the remote shell sh \"env | grep ELASTIC_ACCESS_KEY\" sh \"echo ${ELASTIC_ACCESS_KEY} > secret-file.txt\" } ... } SET TIMEOUT \u00b6 EXAMPLE: SET TIMEOUT timeout for the entire Pipeline (Note the double quotes around DAYS options { timeout ( time: 3 , unit: \"DAYS\" ) } timeout for an \"input\" stage steps { timeout ( time: 3 , unit: \"DAYS\" ) { input ( message: \"Deploy to Stage\" , ok: \"Yes, let's do it!\" ) } } BUT WAIT, I DON\u2019T KNOW THE SYNTAX TO SET THE OPTIONS FOR THE ENTIRE PIPELINE DECLARATIVE DIRECTIVE GENERATOR CAN HELP DECLARATIVE DIRECTIVE GENERATOR CAN HELP EXAMPLE OF PARAMETERS \u00b6 pipeline { agent none parameters { string ( name: 'DEPLOY_ENV' , defaultValue: 'staging' , description: '' ) } stages { stage ( 'Deploy' ) { echo \"Deploying to ${DEPLOY_ENV}\" } } } SUPPORTED TRIGGERS \u00b6 cron\u2009\u2014\u2009Schedule a run at a specified time pollSCM\u2009\u2014\u2009Define a regular interval at which Jenkins should check for new source changes; run the Pipeline if such changes are found upstream\u2009\u2014\u2009Define conditions when a Pipeline should run because of the results of another Pipeline run. See Result class for details about specifying the threshold used. IMPLEMENT A TRIGGER Triggers are specified at the top level of the Pipeline. For example: pipeline { agent any triggers { cron ( 'H */4 * * 1-5' ) } stages { stage ( 'Example' ) { steps { echo 'Hello World' } } } } Credential Usage Example \u00b6 ... stage ( 'Deploy Reports' ) steps { ... withCredentials ( bindings: [ string ( credentialsId: 'elastic-access-key' , variable: 'ELASTIC_ACCESS_KEY' )]) { // Environment Variable available in the remote shell sh 'env | grep ELASTIC_ACCESS_KEY' sh 'echo ${ELASTIC_ACCESS_KEY} > secret-file.txt' // Groovy Variable sh \"echo ${ELASTIC_ACCESS_KEY} > secret-file.txt\" } ... } NOTES ABOUT EXAMPLE CODE Credentials are implemented in a Pipeline using the withCredentials() method Uses the credential that is defined with the ID of elastic-access-key withCredentials binds that to a local Pipeline variable called ELASTIC_ACCESS_KEY The deploy-token step uses the ELASTIC_ACCESS_KEY local Pipeline variable to establish the credentials that it needs Anyone who can run this Pipeline gets the necessary credentials to run deploy-token SIMPLE VALIDATION OF A BACKUP \u00b6 A simple way to validate a full backup is to restore it to a temporary location: Create a directory for the test validation (such as /mnt/backup-test) and restore the backup to that directory Set $JENKINS_HOME to point to this directory, specifying a random HTTP port so you do not collide with the real Jenkins instance: export JENKINS_HOME=/mnt/backup-test Now execute the restored Jenkins instance: java-jar jenkins.war ---httpPort=9999 Jenkins CLI \u00b6 cloudbees-devbox $ export JENKINS_URL=http://jenkins:8080/jenkins cloudbees-devbox $ java -jar jenkins-cli.jar \\ -auth butler:{API Token} help ### No Proxy Setup You can instead configure No Proxy Host using the -Dhttp.nonProxyHosts property in the Master startup script ### FIREWALL CONSIDERATIONS If your CloudBees Core instances run behind a Firewall: The firewall must use raw tcp mode for the CLI port Complex keep-alive functionality on the firewall may interfere with CloudBees Core operations The firewall must not exercise round robin operations on the CLI port RBAC COMPONENTS \u00b6 RBAC = Role-Based Access Control Roles defined by administrators Set of Permissions assigned to Roles Roles assigned to Groups Users belong to Groups Groups defined for Masters, Folders, and Jobs ALTERNATIVES TO RBAC Authorization can be implemented using the standard Jenkins Matrix-based security strategy You must install the Matrix Authorization Strategy Plugin manually If you have a large number of users, this can be very cumbersome to maintain over time Team Masters on CloudBees Core on modern cloud platforms provides a simplified authorization strategy All users with administer authorization on the Operations Center have administer authorization for each Team Master The Administrator for the Team Master defines the users for that Master and whether each user is an Administrator, a Member who can read, write, and execute Pipelines, or a Guest who has read-only access to the Pipelines on this Master ROLE-BASED ACCESS CONTROL rbac simplified BENEFITS OF RBAC RBAC supports granular permissions for roles Similar to Linux file permissions but more extensive For example Browse role has read-only permissions QA role supports read/write/execute permissions Roles and Groups assignable to CloudBees Core objects to apply to contents For example A team-only project folder containing all jobs in a pipeline A QA job only available to the QA team Groups allow for mass permissions management CLOUDBEES RBAC PLUGIN CloudBees Core includes the RBAC plugin RBAC configurable in Authorization Policy Manage Jenkins \u2192 Configure Global Security WHAT RBAC ALLOWS RBAC allows: Global level role assignment Object specific authorizations Roles can apply to all objects Masters Shared agents Jobs Folders Views Roles are selectively inheritable Filter out undesired inheritance from Parent to Child CONFIGURE NON-BLOCKING I/O SSH AGENTS \u00b6 Configured when you create a new agent, shared agent, or shared cloud * Launch agents on Unix machines via SSH (Non-blocking I/O) * Prefix Start Agent Command syntax * Suffix Start Agent Command syntax * Logging of Agent Environment Only minor differences from old SSH configuration PREFIX/SUFFIX START AGENT \u00b6 Prefix Start Agent Command to be prepended to the start agent command You no longer need to remember to include a space Suffix Start Agent Command to be appended to the start agent command You no longer need to remember to include a space LOGGING OF AGENT ENVIRONMENT The traditional SSH connector outputs environment variables to build log NIO SSH makes this optional Off by default Agent environment variables on the node\u2019s System Information screen USING NON-BLOCKING SSH AGENTS ON WINDOWS Works with Cygwin\u2019s OpenSSH Other SSH implementations on Windows not tested See documentation for troubleshooting steps Setting up a reference Windows agent ASSIGN PERMISSIONS ON SHARED NODES \u00b6 In most cases, you will want to grant the same permissions to the same roles you use for regular agents but you have the option of defining different permissions and roles Client Master /Configure - Can modify the configuration and management parts of Client Masters Shared Agent /Configure and Shared Cloud /Configure - Can modify the configuration of shared agents and shared clouds Shared Agent /Connect and Shared Cloud /Connect - Allows an inbound agent to be connected as a shared agent or shared cloud Shared Agent /Disconnect and Shared Cloud /Disconnect - Disconnect an inbound agent from the shared cloud Shared Agent /ForceRelease and Shared Cloud /ForceRelease - Can force a lease of a shared agent into the released state","title":"CICD Interview Questions"},{"location":"interviewQuestions/#cicd-interview-questions","text":"We believe that these questions/exercise will help the candidates to get the right expectations about the type of work that (s)he is goin to get. This will also help us to fitler candidates who are not suitable for the position. **Try to answer maximum questions. Feel free to use Google. Please upload the answers to a github repository and share us the URL. Configure Jenkins job to trigger SonarQube analysis on Pull Requests/Code Reviews? Write a Jenkins pipelin to fetch source from GitHub and publish the (Java/NodeJS) artifacts to Artifactory/nexus repository. Create a container(Docker) image that contains an application server with above build artifacts. Define Deployment, Service yaml/json files to deploy above container image on to Kubernetes/OpenShift platforms. Write a Cookbook/Playbook to host a simple application server with above build artifacts (Question 2). Data Flow?","title":"CICD Interview Questions"},{"location":"interviewQuestions/#branching-statergy","text":"common libraries in Jenkins Multi branch pipeline vs Org scan job HOW TO DO SECURITY? LEAST PRIVILEGE Handling \"Least privileges\" concepts helps you manage the AAA concepts: Authentication\u2009\u2014\u2009Control who can access the system Authorization\u2009\u2014\u2009Control what each user can do on the system Accounting\u2009\u2014\u2009Monitor your system to ensure that only valid processes are executing","title":"Branching Statergy"},{"location":"interviewQuestions/#how-jenkins-executes-a-pipeline","text":"A simple overview of how Jenkins executes a Pipeline helps us understand the security considerations: By default, the Pipeline executes with the full privileges of the Jenkins administrator You can configure Jenkins to execute Pipelines with fewer privileges All of the Pipeline logic, the Groovy conditionals, loops, etc execute on the master Creates a workspace for each build that runs Stores files created for that build The Pipeline calls steps that run on agents","title":"HOW JENKINS EXECUTES A PIPELINE"},{"location":"interviewQuestions/#what-agents-do","text":"The Pipeline calls a series of steps, each of which is a script or command that does the real work and mostly executes using an executor on an agent Agent writes some files to the local node Agent sends data back to the master Agent can also request information from the master What is Declarity pipeline vs scripted pipeline Scripted: sequential execution, using Groovy expressions for flow control Declarative: uses a framework to control execution Explain Jenkins Pipeline JENKINS PIPELINE Jenkins Pipeline is a tool for defining your Continuous Delivery/Deployment flow as code New fundamental \"type\" in Jenkins that conceptualizes and models a Continuous Delivery process Two syntaxes: Scripted: sequential execution, using Groovy expressions for flow control Declarative: uses a framework to control execution Uses the Pipeline DSL (Domain-specific Language) programmatically manipulate Jenkins Objects Captures the entire continuous delivery process as code Pipeline is not a job creation tool like the Job DSL plugin or Jenkins Job Builder What are pipeline benefits PIPELINE BENEFITS (1/2) The pipeline functionality is: Durable: The Jenkins master can restart and the Pipeline continues to run Pausable: can stop and wait for human input or approval Versatile: supports complex real-world CD requirements (fork, join, loop, parallelize) Extensible: supports custom extensions to its \"DSL\" (Domain-specific Language) PIPELINE BENEFITS (2/2) Reduces number of jobs Easier maintenance Decentralization of job configuration Easier specification through code JENKINS VOCABULARY Master: Computer, VM or container where Jenkins is installed and run Serves requests and handles build tasks Agent: (formerly \"slave\") Computer, VM or container that connects to a Jenkins Master Executes tasks when directed by the Master Has a number and scope of operations to perform. Node is sometimes used to refer to the computer, VM or container used for the Master or Agent; be careful because \"Node\" has another meaning for Pipeline Executor: Computational resource for running builds Performs Operations Can run on any Master or Agent, although running builds on masters can degrade performance and opens up serious security vulnerabilities Can be parallelized on a specific Master or Agent JENKINS PIPELINE SECTIONS The Jenkinsfile that defines a Pipeline uses a DSL based on Apache Groovy syntax Is structured in sections, called stages Each stage includes steps steps include the actual tests/commands to run An agent defines where the programs and scripts execute This is illustrated in the following simple Jenkinsfile: pipeline { agent { label 'linux' } stages { stage ( 'MyBuild' ) { steps { sh './jenkins/build.sh' } } stage ( 'MySmalltest' ) { steps { sh './jenkins/smalltest.sh' } } } } DECLARATIVE VERSUS SCRIPTED PIPELINE Two different syntaxes for Pipeline Both are built on same Pipeline subsystem Both definitions are stored in a Jenkinsfile under SCM (\"Pipeline-as-code\") Both rely on the Pipeline DSL, which is based on Apache Groovy syntax Both can use steps built into Pipeline or provided in plugins Both support shared libraries SCRIPTED PIPELINE Executed serially, from top down Relies on Groovy expressions for flow control Requires extensive knowledge of Groovy syntax Very flexible and extensible Limitations are mostly Groovy limitations Power users with complex requirements may need it Novice users can easily mess up the syntax DECLARATIVE PIPELINE Stricter, pre-defined structure Execution can always be resumed after an interruption, no matter what caused the interruption Requires only limited knowledge of Groovy syntax Using Blue Ocean simplifies the Pipeline creation process even more Encourages a declarative programming model Reduces the risk of syntax errors Use the script step to include bits of scripted code within a Declarative Pipeline only when you have needs that are beyond the capabilities of Declarative syntax PIPELINE CONCEPTS Pipeline is \"glue\" code that binds tasks together into a CD flow. Use the Pipeline DSL for CI/CD scripting only! Do not code arbitrary networking and computational tasks into the Pipeline itself Use tools like Maven, Gradle, NPM, Ant and Make to do the majority of the build \u201cwork\u201d Use scripts written in Shell, Batch, or Python for other related tasks Then call these executables as steps in your Pipeline Declarative Pipeline keeps the complex logic of individual steps separate from the Pipeline itself, making the Pipeline code clean to read What is Archive Artifacts post stage in Jenkins Pipeline Use the pipeline step archiveArtifacts Requires a pattern to know which artifacts to archive my-app.zip: The file my-app.zip, at the workspace\u2019s root images/*.png: All files with .png extension in the images folder at the workspace\u2019s root target/**/*.jar: All files with .jar extension, recursively under the target folder, at the workspace\u2019s root Archiving keeps those files in ${JENKINS_HOME} forever unless you delete them FINGERPRINTS A fingerprint is the MD5 checksum of an artifact Each archived artifact can be fingerprinted; merely check the \"Fingerprint\" box when you create the archiving step Jenkins uses Fingerprints to keep track of artifacts without any ambiguity A database of all fingerprints is managed on the Master Located in the ${JENKINS_HOME}/fingerprints directory USING ARCHIVED ARTIFACTS In a production environment, your build chain system (Maven, Gradle, Make, etc.) publishes artifacts to an artifact repository such as Artifactory, Nexus, etc. Teams can also deploy artifacts from Jenkins to test environments Use the Copy Artifact Plugin to take artifacts from one project (Pipeline run) and copy them to another Project ARTIFACT RETENTION POLICY Coupled to build retention policy Deleting a build deletes attached artifacts Good practice: discard build and clean it Driven by age: # days to keep a build Driven by number: Max # of builds to keep Important builds can individually be Kept Forever WHAT IS JUNIT? As an external application, JUnit is a common testing framework for Java programs In the Jenkins context, JUnit is a publisher that consumes XML test reports Generates some graphical visualization of the historical test results Provides a web UI for viewing test reports, tracking failures and so forth Useful for tracking test result trends Works with all supported build tools You must specify the appropriate path name for the XML test reports generated by the build tool you are using JUnit in this course means the Jenkins publisher PARALLEL STAGES Stages can be run in parallel: Long running stages Builds for different target architectures or OSes (Debian/Fedora, Android/iOS) or different software versions (JVM8/11), et cetera Builds of independent modules of your code Unit and integration tests are tied to the code; run them in parallel DECLARATIVE PARALLEL SYNTAX Each \"parallelized branch\" is a stage A stage can use either steps or parallel at the top level; it cannot use both A stage within a parallel stage can contain agent and steps sections A parallel stage cannot contain agent or tools Add failfast true to force all parallel processes to abort if one fails HOW PARALLEL STAGES ARE SCHEDULED By default, Jenkins tries to allocate a stage to the last node on which it (or a similar stage) executed. This may mean that multiple parallel steps execute on the same node while other nodes in the cluster are idle Use the Least Load Plugin to replace the default load balancer with one that schedules a new stage to nodes with the least load SCRIPTED PIPELINES IN PRODUCTION This exercise is a short introduction to Scripted Pipelines Scripted Pipelines used in production have the following characteristics: Jenkinsfile must be created and modified in a text editor Should be Multibranch Pipeline Requires Git, Bitbucket or SVN Must be configured to the SCM repository, with credentials, hooks, etc Must be committed to the SCM like any other code WHAT IS MULTIBRANCH? Recommended for all new Pipelines All Pipelines created with Blue Ocean are multibranch Requires Git, Bitbucket or SVN Configures a set of branches Jenkins creates a subproject for each branch in the SCM repository WHY MULTIBRANCH Automatic Workflow creation for each new branch in the repository Assumes that webhooks are registered from the SCM to Jenkins Builds are specific to that child/branch and its unique SCM change and build history Automatic job pruning/deletion for branches deleted from the repository, according to the settings Ability to override the parent properties for a specific branch, if necessary SPECIFY DOCKER AGENT IN PIPELINE agent { docker } Execute the Pipeline or stage with the specified container The container will be dynamically provisioned on the node agent { dockerfile } Execute the Pipeline or stage with a container built from the specified Dockerfile DOCKER CONTAINER DURABILITY When you specify a docker container for an agent, Jenkins calls APIs directly These commands are serialized so they can resume after a master restart When you use a shell step to run a Docker command directly, the step is bound to the durable task of the shell The Docker container and any tasks running in it are terminated when the shell terminates SIMPLE SYNTAX Run all Pipeline stages on any node machine: ``` groovy pipeline { agent any .... } Run all Pipeline stages on a node machine tagged as bzzzmaven: pipeline { agent { label 'bzzzmaven' } .... } ``` Run all Pipeline stages on a container based on the image bzzzcentos:7: ``` groovy pipeline { agent { docker 'bzzzcentos:7' } .... } ``` PER-STAGE AGENT SYNTAX Do not run stages on an agent by default Run the stage Build on a node machine tagged as bzzzmaven Run the stage Deploy on a node machine tagged as bzzzproduction: pipeline { agent none stages { stage ( 'Build' ) { agent { label 'bzzzmaven' } steps { ... } } stage ( 'Deploy' ) { agent { label 'bzzzproduction' } steps { ... } } } } SPECIFY AGENTS FOR OUR PIPELINE We are building our application to work with both JDK7 and JDK8. This means that we need a JDK7 environment and a JDK8 environment. To implement this: * Change the global agent to be agent none; in other words, there is no global agent * Specify the appropriate agent (labeled either jdk7 or jdk8) for the build and test steps that need the specific JDK environment. * Specify agent any for stages that do not require a specific JDK version. STASH/UNSTASH Use stash to save a set of files for use later in the same build, but in another stage that executes on another node/workspace Use unstash to restore the stashed files into the current workspace of the other stage Stashed files are discarded at the end of the build Stashed files are archived in a compressed tar file, which works well for small (<5 Mb) files Stashing large files consumes significant resources (especially CPU) on both the Master and agent nodes For large files, consider using: the External Workspace manager plugin an external repository manager such as Nexus or Artifactory the Artifact Manager on S3 plugin (README) CALLING SYNTAX * stash and unstash are implemented as steps within a stage * stash requires one parameter, name, which is a simple identifier for the set of files being stashed * By default, stash places all workspace files into the named stash * To store files from a different directory (or a subset of the workspace): - Use the optional includes parameter to give the name of the files or directories to store. This accepts a set of Ant-style include patterns. - Other optional parameters are documented on the Pipeline: Basic Steps reference page * To unstash files: - Optionally, use the `dir` step to create a directory where the files will be written. - Call `unstash`, using the name you assigned with the stash step above. IMPLEMENT STASH/UNSTASH (\u00bd) * We are going to make our Pipeline build and test our software for both JDK 7 and JDK 8. - We will use stash/unstash to ensure that the JDK 7 tests are run against the JDK 7 build and that the JDK 8 tests are run against the JDK 8 build: * Add a step to the \"Build Java 7\" stage: \"Stash some files to be used later in the build\" - Name the stash \"Buzz Java 7\"; enter target/** in the \"Includes\" box to save everything under the target directory * Follow the same procedure to stash \"Buzz Java 8\" in the \"Build Java 8\" stage CODE: WAIT FOR INPUT The code in the Jenkinsfile looks like: stage ( 'Confirm Deploy to Staging' ) { agent none steps { input ( message: 'Deploy to Stage' , ok: \"Yes, let's do it!\" ) } } Note that Blue Ocean uses single quotes to enclose the text in the \"Message\" and \"OK\" boxes and automatically escapes apostrophes in the text To make the code cleaner, you can use the code editor to use double quotes to enclose the text Sample pipeline SAMPLE FINISHING PIPEINE pipeline { agent none stages { stage ( 'Fluffy Build' ) { parallel { stage ( 'Build Java 8' ) { agent { node { label 'java8' } } steps { sh './jenkins/build.sh' } post { success { stash ( name: 'Java 8' , includes: 'target/**' ) } } } stage ( 'Build Java 7' ) { agent { node { label 'java7' } } steps { sh './jenkins/build.sh' } post { success { archiveArtifacts 'target/*.jar' stash ( name: 'Java 7' , includes: 'target/**' ) } } } } } stage ( 'Fluffy Test' ) { parallel { stage ( 'Backend Java 8' ) { agent { node { label 'java8' } } steps { unstash 'Java 8' sh './jenkins/test-backend.sh' } post { always { junit 'target/surefire-reports/**/TEST*.xml' } } } stage ( 'Frontend' ) { agent { node { label 'java8' } } steps { unstash 'Java 8' sh './jenkins/test-frontend.sh' } post { always { junit 'target/test-results/**/TEST*.xml' } } } stage ( 'Performance Java 8' ) { agent { node { label 'java8' } } steps { unstash 'Java 8' sh './jenkins/test-performance.sh' } } stage ( 'Static Java 8' ) { agent { node { label 'java8' } } steps { unstash 'Java 8' sh './jenkins/test-static.sh' } } stage ( 'Backend Java 7' ) { agent { node { label 'java7' } } steps { unstash 'Java 7' sh './jenkins/test-backend.sh' } post { always { junit 'target/surefire-reports/**/TEST*.xml' } } } stage ( 'Frontend Java 7' ) { agent { node { label 'java7' } } steps { unstash 'Java 7' sh './jenkins/test-frontend.sh' } post { always { junit 'target/test-results/**/TEST*.xml' } } } stage ( 'Performance Java 7' ) { agent { node { label 'java7' } } steps { unstash 'Java 7' sh './jenkins/test-performance.sh' } } stage ( 'Static Java 7' ) { agent { node { label 'java7' } } steps { unstash 'Java 7' sh './jenkins/test-static.sh' } } } } stage ( 'Confirm Deploy' ) { when { branch 'master' } steps { input ( message: 'Okay to Deploy to Staging?' , ok: 'Let\\'s Do it!' ) } } stage ( 'Fluffy Deploy' ) { when { branch 'master' } agent { node { label 'java7' } } steps { unstash 'Java 7' sh \"./jenkins/deploy.sh ${params.DEPLOY_TO}\" } } } parameters { string ( name: 'DEPLOY_TO' , defaultValue: 'dev' , description: '' ) } } SAMPLE POST CODE pipeline { stages { stage ( 'Buzz Build' ) { parallel { stage ( 'Build Java 7' ) { steps { sh \"\"\" echo I am a $BUZZ_NAME! ./jenkins/build.sh \"\"\" } post { always { archiveArtifacts ( artifacts: 'target/*.jar' , fingerprint: true ) } success { stash ( name: 'Buzz Java 7' , includes: 'target/**' ) } } } ... } WHAT IS AN ENVIRONMENT DIRECTIVE? Specifies a sequence of key-value pairs that are defined as environment variables Can be specified globally for the Pipeline, to apply to all steps Can be specified for an individual stage to apply only to that stage By manually coding withEnv inside a steps block, a specific environment variable can be specified for one or more (but not all) steps within a stage SINGLE AND DOUBLE QUOTES The Pipeline DSL uses Apache Groovy syntax. Variables are dereferenced according to whether they are enclosed in single quotes or double quotes: Single quotes: The dereferencing syntax (${VARIABLE_NAME}) is passed literally to the Pipeline sh step; the shell interpreter on the agent dereferences the variable. Double quotes: The Pipeline code dereferences the variable on the master\u2019s JVM thread then passes the calculated string to the sh step. ENVIRONMENT EXAMPLE pipeline { agent any environment { CC = 'clang' } stages { stage('Example') { environment { AN_ACCESS_KEY = credentials('my-predefined-secret-text') } steps { sh 'printenv' The first use of environment applies to the entire Pipeline The second use of environment applies only to the Example stage NOTIFICATIONS EXAMPLE NOTIFICATIONS WHEN BUILD STARTS (\u00bd) stages { stage ( 'Start' ) { steps { // send build started notifications slackSend ( color: '#FFFF00' , message: \"STARTED: Job '${env.JOB_NAME} [${env.BUILD_NUMBER}]' (${env.BUILD_URL})\" ) } } } NOTIFICATIONS WHEN BUILD STARTS (2/2) /* ... unchanged ... */ // send to email emailext ( subject: \"STARTED: Job '${env.JOB_NAME} [${env.BUILD_NUMBER}]'\" , body: \"\"\"<p>STARTED: Job '${env.JOB_NAME} [${env.BUILD_NUMBER}]':</p> <p>Check console output at &QUOT;<a href='${env.BUILD_URL}'>${env.JOB_NAME} [${env.BUILD_NUMBER}]</a>&QUOT;</p>\"\"\" , recipientProviders: [[ $class : 'DevelopersRecipientProvider' ]] ) /* ... unchanged ... */ NOTIFICATIONS WHEN BUILD SUCCEEDS post { success { slackSend ( color: '#00FF00' , message: \"SUCCESSFUL: Job '${env.JOB_NAME} [${env.BUILD_NUMBER}]' (${env.BUILD_URL})\" ) emailext ( subject: \"SUCCESSFUL: Job '${env.JOB_NAME} [${env.BUILD_NUMBER}]'\" , body: \"\"\"<p>SUCCESSFUL: Job '${env.JOB_NAME} [${env.BUILD_NUMBER}]':</p> <p>Check console output at &QUOT;<a href='${env.BUILD_URL}'>${env.JOB_NAME} [${env.BUILD_NUMBER}]</a>&QUOT;</p>\"\"\" , recipientProviders: [[ $class : 'DevelopersRecipientProvider' ]] ) } } NOTIFICATIONS WHEN BUILD FAILS failure { slackSend ( color: '#FF0000' , message: \"FAILED: Job '${env.JOB_NAME} [${env.BUILD_NUMBER}]' (${env.BUILD_URL})\" ) emailext ( subject: \"FAILED: Job '${env.JOB_NAME} [${env.BUILD_NUMBER}]'\" , body: \"\"\"<p>FAILED: Job '${env.JOB_NAME} [${env.BUILD_NUMBER}]':</p> <p>Check console output at &QUOT;<a href='${env.BUILD_URL}'>${env.JOB_NAME} [${env.BUILD_NUMBER}]</a>&QUOT;</p>\"\"\" , recipientProviders: [[ $class : 'DevelopersRecipientProvider' ]] ) } CONFIGURE NOTIFICATIONS * Your Jenkins administrator must configure notifications for Jenkins in order for the notifications to be delivered. * Slack uses an API token - Integration points for each generate the token - That generated token must be copied into the Jenkins configuration * The Slack configuration provides a button that can be used to test the configured connection","title":"WHAT AGENTS DO"},{"location":"interviewQuestions/#when-directive","text":"","title":"WHEN DIRECTIVE"},{"location":"interviewQuestions/#what-is-the-when-directive","text":"Specifies conditions that must be met for Pipeline to execute the stage Must contain at least one condition Supports nested conditions","title":"WHAT IS THE \"WHEN\" DIRECTIVE?"},{"location":"interviewQuestions/#built-in-conditions","text":"branch \u2009\u2014\u2009Execute the stage when the branch being built matches the branch pattern given: when { branch 'master' } environment \u2009\u2014\u2009Execute the stage when the specified environment variable is set to the given value: when { environment name: 'DEPLOY_TO' , value: 'production' } expression \u2009\u2014\u2009Execute the stage when the specified expression evaluates to true: when { expression { return params . DEBUG_BUILD } }","title":"BUILT-IN CONDITIONS"},{"location":"interviewQuestions/#built-in-nested-conditions","text":"allOf\u2009\u2014\u2009Execute the stage when all nested conditions are true: when { allOf { branch 'master' environment name: 'DEPLOY_TO', value: 'production' // AND } } anyOf\u2009\u2014\u2009Execute the stage when at least one of the nested conditions is true when { anyOf { branch 'master' branch 'staging' // OR } } not\u2009\u2014\u2009Execute the stage when the nested condition is false. `when { not { branch 'master' } }`","title":"BUILT-IN NESTED CONDITIONS"},{"location":"interviewQuestions/#multiple-conditions","text":"If the when directive contains more than one condition, all specified conditions must return true for the stage to execute. This is identical to using the allOf condition","title":"MULTIPLE CONDITIONS"},{"location":"interviewQuestions/#implementing-when-directive","text":"The \"Confirm Deploy to Staging\" and \"Deploy to Staging\" steps are only appropriate when building from the master branch. Use Ctrl+S (Cmd+S on macOS) to open the Blue Ocean code editor and add when directives that specify the branch condition to these two stages. The Blue Ocean Editor does not show the when directive but it is in the Jenkinsfile. When we save and run the Pipeline, these steps are skipped because we are not working on the master branch.","title":"IMPLEMENTING \"WHEN\" DIRECTIVE"},{"location":"interviewQuestions/#when-directive-example","text":"... stage ( 'Confirm Deploy to Staging' ) { when { branch 'master' } steps { input ( message: 'Deploy to Stage' , ok: 'Yes, let\\'s do it!' ) } } stage ( 'Deploy to Staging' ) { agent { node { label 'java8' } } when { branch 'master' } steps { unstash 'Buzz Java 8' sh './jenkins/deploy.sh staging' } } ...","title":"\"WHEN\" DIRECTIVE EXAMPLE"},{"location":"interviewQuestions/#credentials-example-code","text":"... stage ( 'Deploy Reports' ) steps { ... withCredentials ( bindings: [ string ( credentialsId: 'my-elastic-key' , variable: 'ELASTIC_ACCESS_KEY' )]) { // Environment Variable available in the remote shell sh \"env | grep ELASTIC_ACCESS_KEY\" sh \"echo ${ELASTIC_ACCESS_KEY} > secret-file.txt\" } ... }","title":"CREDENTIALS EXAMPLE CODE"},{"location":"interviewQuestions/#set-timeout","text":"EXAMPLE: SET TIMEOUT timeout for the entire Pipeline (Note the double quotes around DAYS options { timeout ( time: 3 , unit: \"DAYS\" ) } timeout for an \"input\" stage steps { timeout ( time: 3 , unit: \"DAYS\" ) { input ( message: \"Deploy to Stage\" , ok: \"Yes, let's do it!\" ) } } BUT WAIT, I DON\u2019T KNOW THE SYNTAX TO SET THE OPTIONS FOR THE ENTIRE PIPELINE DECLARATIVE DIRECTIVE GENERATOR CAN HELP DECLARATIVE DIRECTIVE GENERATOR CAN HELP","title":"SET TIMEOUT"},{"location":"interviewQuestions/#example-of-parameters","text":"pipeline { agent none parameters { string ( name: 'DEPLOY_ENV' , defaultValue: 'staging' , description: '' ) } stages { stage ( 'Deploy' ) { echo \"Deploying to ${DEPLOY_ENV}\" } } }","title":"EXAMPLE OF PARAMETERS"},{"location":"interviewQuestions/#supported-triggers","text":"cron\u2009\u2014\u2009Schedule a run at a specified time pollSCM\u2009\u2014\u2009Define a regular interval at which Jenkins should check for new source changes; run the Pipeline if such changes are found upstream\u2009\u2014\u2009Define conditions when a Pipeline should run because of the results of another Pipeline run. See Result class for details about specifying the threshold used. IMPLEMENT A TRIGGER Triggers are specified at the top level of the Pipeline. For example: pipeline { agent any triggers { cron ( 'H */4 * * 1-5' ) } stages { stage ( 'Example' ) { steps { echo 'Hello World' } } } }","title":"SUPPORTED TRIGGERS"},{"location":"interviewQuestions/#credential-usage-example","text":"... stage ( 'Deploy Reports' ) steps { ... withCredentials ( bindings: [ string ( credentialsId: 'elastic-access-key' , variable: 'ELASTIC_ACCESS_KEY' )]) { // Environment Variable available in the remote shell sh 'env | grep ELASTIC_ACCESS_KEY' sh 'echo ${ELASTIC_ACCESS_KEY} > secret-file.txt' // Groovy Variable sh \"echo ${ELASTIC_ACCESS_KEY} > secret-file.txt\" } ... } NOTES ABOUT EXAMPLE CODE Credentials are implemented in a Pipeline using the withCredentials() method Uses the credential that is defined with the ID of elastic-access-key withCredentials binds that to a local Pipeline variable called ELASTIC_ACCESS_KEY The deploy-token step uses the ELASTIC_ACCESS_KEY local Pipeline variable to establish the credentials that it needs Anyone who can run this Pipeline gets the necessary credentials to run deploy-token","title":"Credential Usage Example"},{"location":"interviewQuestions/#simple-validation-of-a-backup","text":"A simple way to validate a full backup is to restore it to a temporary location: Create a directory for the test validation (such as /mnt/backup-test) and restore the backup to that directory Set $JENKINS_HOME to point to this directory, specifying a random HTTP port so you do not collide with the real Jenkins instance: export JENKINS_HOME=/mnt/backup-test Now execute the restored Jenkins instance: java-jar jenkins.war ---httpPort=9999","title":"SIMPLE VALIDATION OF A BACKUP"},{"location":"interviewQuestions/#jenkins-cli","text":"cloudbees-devbox $ export JENKINS_URL=http://jenkins:8080/jenkins cloudbees-devbox $ java -jar jenkins-cli.jar \\ -auth butler:{API Token} help ### No Proxy Setup You can instead configure No Proxy Host using the -Dhttp.nonProxyHosts property in the Master startup script ### FIREWALL CONSIDERATIONS If your CloudBees Core instances run behind a Firewall: The firewall must use raw tcp mode for the CLI port Complex keep-alive functionality on the firewall may interfere with CloudBees Core operations The firewall must not exercise round robin operations on the CLI port","title":"Jenkins CLI"},{"location":"interviewQuestions/#rbac-components","text":"RBAC = Role-Based Access Control Roles defined by administrators Set of Permissions assigned to Roles Roles assigned to Groups Users belong to Groups Groups defined for Masters, Folders, and Jobs ALTERNATIVES TO RBAC Authorization can be implemented using the standard Jenkins Matrix-based security strategy You must install the Matrix Authorization Strategy Plugin manually If you have a large number of users, this can be very cumbersome to maintain over time Team Masters on CloudBees Core on modern cloud platforms provides a simplified authorization strategy All users with administer authorization on the Operations Center have administer authorization for each Team Master The Administrator for the Team Master defines the users for that Master and whether each user is an Administrator, a Member who can read, write, and execute Pipelines, or a Guest who has read-only access to the Pipelines on this Master ROLE-BASED ACCESS CONTROL rbac simplified BENEFITS OF RBAC RBAC supports granular permissions for roles Similar to Linux file permissions but more extensive For example Browse role has read-only permissions QA role supports read/write/execute permissions Roles and Groups assignable to CloudBees Core objects to apply to contents For example A team-only project folder containing all jobs in a pipeline A QA job only available to the QA team Groups allow for mass permissions management CLOUDBEES RBAC PLUGIN CloudBees Core includes the RBAC plugin RBAC configurable in Authorization Policy Manage Jenkins \u2192 Configure Global Security WHAT RBAC ALLOWS RBAC allows: Global level role assignment Object specific authorizations Roles can apply to all objects Masters Shared agents Jobs Folders Views Roles are selectively inheritable Filter out undesired inheritance from Parent to Child","title":"RBAC COMPONENTS"},{"location":"interviewQuestions/#configure-non-blocking-io-ssh-agents","text":"Configured when you create a new agent, shared agent, or shared cloud * Launch agents on Unix machines via SSH (Non-blocking I/O) * Prefix Start Agent Command syntax * Suffix Start Agent Command syntax * Logging of Agent Environment Only minor differences from old SSH configuration","title":"CONFIGURE NON-BLOCKING I/O SSH AGENTS"},{"location":"interviewQuestions/#prefixsuffix-start-agent","text":"Prefix Start Agent Command to be prepended to the start agent command You no longer need to remember to include a space Suffix Start Agent Command to be appended to the start agent command You no longer need to remember to include a space LOGGING OF AGENT ENVIRONMENT The traditional SSH connector outputs environment variables to build log NIO SSH makes this optional Off by default Agent environment variables on the node\u2019s System Information screen USING NON-BLOCKING SSH AGENTS ON WINDOWS Works with Cygwin\u2019s OpenSSH Other SSH implementations on Windows not tested See documentation for troubleshooting steps Setting up a reference Windows agent","title":"PREFIX/SUFFIX START AGENT"},{"location":"interviewQuestions/#assign-permissions-on-shared-nodes","text":"In most cases, you will want to grant the same permissions to the same roles you use for regular agents but you have the option of defining different permissions and roles Client Master /Configure - Can modify the configuration and management parts of Client Masters Shared Agent /Configure and Shared Cloud /Configure - Can modify the configuration of shared agents and shared clouds Shared Agent /Connect and Shared Cloud /Connect - Allows an inbound agent to be connected as a shared agent or shared cloud Shared Agent /Disconnect and Shared Cloud /Disconnect - Disconnect an inbound agent from the shared cloud Shared Agent /ForceRelease and Shared Cloud /ForceRelease - Can force a lease of a shared agent into the released state","title":"ASSIGN PERMISSIONS ON SHARED NODES"},{"location":"jenkins/","text":"Jenkins \u00b6 List Credentilas \u00b6 List credentails of jenkins from global \u00b6 Script to include SSH credentials contained within folders. Run in script console import com.cloudbees.plugins.credentials.Credentials Set<Credentials> allCredentials = new HashSet<Credentials> () ; def creds = com.cloudbees.plugins.credentials.CredentialsProvider.lookupCredentials ( com.cloudbees.jenkins.plugins.sshcredentials.impl.BasicSSHUserPrivateKey ) ; for ( c in creds ) { println ( c.id + \": \\n passphrase: \" + c.getPassphrase () + \"\\n\" + c.getPrivateKey ()) } List all credentails of all folder import com.cloudbees.plugins.credentials.Credentials Set<Credentials> allCredentials = new HashSet<Credentials> () ; def creds = com.cloudbees.plugins.credentials.CredentialsProvider.lookupCredentials ( com.cloudbees.jenkins.plugins.sshcredentials.impl.BasicSSHUserPrivateKey ) ; allCredentials.addAll ( creds ) Jenkins.instance.getAllItems ( com.cloudbees.hudson.plugins.folder.Folder.class ) .each { f -> creds = com.cloudbees.plugins.credentials.CredentialsProvider.lookupCredentials ( com.cloudbees.jenkins.plugins.sshcredentials.impl.BasicSSHUserPrivateKey, f ) allCredentials.addAll ( creds ) } for ( c in allCredentials ) { println ( c.id + \": \\n passphrase: \" + c.getPassphrase () + \"\\n\" + c.getPrivateKey ()) } Here are some Video Tutorials and additional learning materials . There are great resources of groovy scripts that you can review and use for inspiration at Example Groovy scripts . The Javadocs for the APIs that you can call in a groovy script can be found at: https://javadoc.jenkins.io/ https://javadoc.jenkins.io/plugin/ Custom Plugins: APIs and Javadocs of CloudBees Jenkins Enterprise plugins","title":"Jenkins"},{"location":"jenkins/#jenkins","text":"","title":"Jenkins"},{"location":"jenkins/#list-credentilas","text":"","title":"List Credentilas"},{"location":"jenkins/#list-credentails-of-jenkins-from-global","text":"Script to include SSH credentials contained within folders. Run in script console import com.cloudbees.plugins.credentials.Credentials Set<Credentials> allCredentials = new HashSet<Credentials> () ; def creds = com.cloudbees.plugins.credentials.CredentialsProvider.lookupCredentials ( com.cloudbees.jenkins.plugins.sshcredentials.impl.BasicSSHUserPrivateKey ) ; for ( c in creds ) { println ( c.id + \": \\n passphrase: \" + c.getPassphrase () + \"\\n\" + c.getPrivateKey ()) } List all credentails of all folder import com.cloudbees.plugins.credentials.Credentials Set<Credentials> allCredentials = new HashSet<Credentials> () ; def creds = com.cloudbees.plugins.credentials.CredentialsProvider.lookupCredentials ( com.cloudbees.jenkins.plugins.sshcredentials.impl.BasicSSHUserPrivateKey ) ; allCredentials.addAll ( creds ) Jenkins.instance.getAllItems ( com.cloudbees.hudson.plugins.folder.Folder.class ) .each { f -> creds = com.cloudbees.plugins.credentials.CredentialsProvider.lookupCredentials ( com.cloudbees.jenkins.plugins.sshcredentials.impl.BasicSSHUserPrivateKey, f ) allCredentials.addAll ( creds ) } for ( c in allCredentials ) { println ( c.id + \": \\n passphrase: \" + c.getPassphrase () + \"\\n\" + c.getPrivateKey ()) } Here are some Video Tutorials and additional learning materials . There are great resources of groovy scripts that you can review and use for inspiration at Example Groovy scripts . The Javadocs for the APIs that you can call in a groovy script can be found at: https://javadoc.jenkins.io/ https://javadoc.jenkins.io/plugin/ Custom Plugins: APIs and Javadocs of CloudBees Jenkins Enterprise plugins","title":"List credentails of jenkins from global"},{"location":"linux/","text":"Linux Cheat Sheet \u00b6 Checksum file \u00b6 MD5SUM <file-name> sha256sum <file-name> in Mac shasum -a 256 <file-name>","title":"Linux Cheat Sheet"},{"location":"linux/#linux-cheat-sheet","text":"","title":"Linux Cheat Sheet"},{"location":"linux/#checksum-file","text":"MD5SUM <file-name> sha256sum <file-name> in Mac shasum -a 256 <file-name>","title":"Checksum file"},{"location":"mkdocs/","text":"Setting up MKDocs \u00b6 Pre-requisite \u00b6 Setup local working environment > home dir cd Tools python3 -m venv $HOME/Tools/.venv source $HOME/Tools/.venv/bin/activate python3 -m pip install --requirement=Documents/GitHub/Documents/requirements.txt // Enterprise option python3 -m pip install --index-url=https://nexus.com/repository/pypi-proxy/simple \\ --trusted-host=nexus.com \\ --index=https://nexus.com/repository/pypi-proxy/pip \\ --requirement=requirements.txt python3 -m pip install mkdocs-git-revision-date-localized-plugin==0.5.2 python3 -m pip install mkdocs-redirects pip install mkdocs-minify-plugin python3 -m mkdocs serve Open http://127.0.0.1:8000","title":"Setting up MKDocs"},{"location":"mkdocs/#setting-up-mkdocs","text":"","title":"Setting up MKDocs"},{"location":"mkdocs/#pre-requisite","text":"Setup local working environment > home dir cd Tools python3 -m venv $HOME/Tools/.venv source $HOME/Tools/.venv/bin/activate python3 -m pip install --requirement=Documents/GitHub/Documents/requirements.txt // Enterprise option python3 -m pip install --index-url=https://nexus.com/repository/pypi-proxy/simple \\ --trusted-host=nexus.com \\ --index=https://nexus.com/repository/pypi-proxy/pip \\ --requirement=requirements.txt python3 -m pip install mkdocs-git-revision-date-localized-plugin==0.5.2 python3 -m pip install mkdocs-redirects pip install mkdocs-minify-plugin python3 -m mkdocs serve Open http://127.0.0.1:8000","title":"Pre-requisite"}]}